<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
</head>
<body>
<div id="layout-content">
<p>
</p>
<p>
<div style="text-align:right;">
<p>
<a href="index.html"><font color="black">home</font></a>    <span style="margin-left:20px;"></span> | <span style="margin-left:20px;"></span>   <a href="publications.html"><font color="black">publications</font> <span style="margin-left:100px;"></span> </a>
</p>
</div>









</p>
<h1><b>Viet-Hoang Tran</b></h1>
<p>
I'm currently a fourth-year Ph.D. student in the <a href="https://www.math.nus.edu.sg/" target=&ldquo;blank&rdquo;>Department of Mathematics</a>, <a href="https://nus.edu.sg/" target=&ldquo;blank&rdquo;>National University of Singapore</a>. 

I began my Ph.D. in August 2022 in Mathematics.

In September 2023, I shifted my research focus to Machine Learning and started working under the supervision of Professor <a href="https://tanmnguyen89.github.io/" target=&ldquo;blank&rdquo;>Tan Minh Nguyen</a>.

<br />

<br />

Previously, I completed an Honors degree in Mathematics at the <a href="https://english.hus.vnu.edu.vn/" target=&ldquo;blank&rdquo;>Hanoi University of Science</a> (2018-2022). <br />

<br />

email: hoang.tranviet [at] u.nus.edu <span style="margin-left:15px;"></span>  | <span style="margin-left:15px;"></span>  <a href="https://scholar.google.com/citations?hl=en&amp;user=HXsV5dQAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" target=&ldquo;blank&rdquo;>Google Scholar</a> 








</p>
<h2><b>Research Interests</b></h2>
<p>
</p>
<ul>
<li><p><b>Symmetry of Parameter Space.</b> This research direction aims to completely characterize the symmetries inherent in modern neural architectures and to translate these theoretical insights into practical methodologies, including the  analysis of (linear) mode connectivity, the design of symmetry-respecting optimizers, and the development of equivariant metanetworks.

<br />

</p>
</li>
<li><p><b>Efficient methods based on Optimal Transport (OT).</b> Our research develops scalable OT-based techniques, with a particular focus on tree-sliced OT, a generalization of sliced OT that exploits closed-form solutions of OT problems on tree metric spaces. <br />

</p>
</li>
</ul>
<p>

keywords: machine learning, parameter space symmetry, mode connectivity, metanetwork, optimal transport







</p>
<h2><b>News</b></h2>
<p>
</p>
<ul>
<li><p>Jan 2026. 4 papers <a href="https://openreview.net/forum?id=XMiDpi2mWY" target=&ldquo;blank&rdquo;><b>Quasi-Equivariant Metanetworks</b></a>, <a href="https://openreview.net/forum?id=e439wJl5sT" target=&ldquo;blank&rdquo;><b>Mixed-Curvature Tree-Sliced Wasserstein Distance</b></a>, <a href="https://openreview.net/forum?id=HHNQSXaLkF" target=&ldquo;blank&rdquo;><b>Tree-sliced Sobolev IPM</b></a>, and <a href="https://openreview.net/forum?id=kDqG03v05B" target=&ldquo;blank&rdquo;><b>Revisiting Tree-Sliced Wasserstein Distance Through the Lens of the Fermat-Weber Problem</b></a>, are accepted at ICLR 2026

</p>
</li>
<li><p>Oct 2025. 1 paper <a href="https://openreview.net/forum?id=aUTKbAfgzn" target=&ldquo;blank&rdquo;><b>Modeling Expert Interactions in Sparse Mixture of Experts via Graph Structures</b></a> is accepted at AAAI 2026 Workshop MATH4AI

</p>
</li>
<li><p>Sep 2025. 3 papers <a href="https://arxiv.org/abs/2509.11348" target=&ldquo;blank&rdquo;><b>On Linear Mode Connectivity of Mixture-of-Experts Architectures</b></a> <b>(oral)</b>, <a href="https://openreview.net/forum?id=41ZbysfW4h" target=&ldquo;blank&rdquo;><b>Tree-Sliced Entropy Partial Transport</b></a>, and <a href="https://openreview.net/forum?id=i9Vn8vV99g" target=&ldquo;blank&rdquo;><b>Dynamical Properties of Tokens in Self-Attention and Effects of Positional Encoding</b></a>, are accepted at NeurIPS 2025

</p>
</li>
<li><p>Jun 2025. I will give a talk on  <a><b>Functional Equivalence in Neural Architectures</b></a>, in Joint FDU-NUS-ZJU PhD Forum in Mathematics, at Department of Mathematics, National University of Singapore

</p>
</li>
<li><p>May 2025. 3 papers <a href="https://arxiv.org/abs/2410.04213" target=&ldquo;blank&rdquo;><b>Equivariant Polynomial Functional Networks</b></a>, <a href="https://arxiv.org/abs/2406.13725" target=&ldquo;blank&rdquo;><b>Tree-Sliced Wasserstein Distance: A Geometric Perspective</b></a>, and <a href="https://arxiv.org/abs/2505.00968" target=&ldquo;blank&rdquo;><b>Tree-Sliced Wasserstein Distance with Nonlinear Projection</b></a>, are accepted at ICML 2025

</p>
</li>
</ul>
<p>
<details>
<summary><font color="black"> <span style="font-size: 0.7em; font-weight: bold;text-shadow: 0.7px 0 0 currentColor,
0.7px 0 0 currentColor,
0 0.7px 0 currentColor,
0 0.7px 0 currentColor;"> > <span style="margin-left:7px;"></span> </span> 
</font>
<font color="black"> <span style="font-size: 80%;"> older news <span style="margin-left:7px;"></span> </span> 
</font>
</summary>
<ul style="line-height:1.4; margin-top:0.5em;">
<li>Apr 2025. I will give a contributed talk on our recent work 
<a href="https://arxiv.org/abs/2410.04209"> <b> Equivariant Neural Functional Networks for Transformers </b> </a> 
at ICLR 2025 Weight Space Learning Workshop
</li>

<li>Feb 2025. 1 paper 
<a href="https://arxiv.org/abs/2410.04209"> <b> Equivariant Neural Functional Networks for Transformers </b> </a> 
<b>(spotlight)</b> is accepted at ICLR 2025 Workshop Weight Space Learning
</li>

<li>Jan 2025. 3 papers 
<a href="https://arxiv.org/abs/2410.04209"><b> Equivariant Neural Functional Networks for Transformers </b> </a>, 
<a href="https://arxiv.org/abs/2503.11050"><b> Distance-Based Tree-Sliced Wasserstein Distance </b> </a>, and 
<a href="https://arxiv.org/abs/2503.11249"><b> Spherical Tree-Sliced Wasserstein Distance </b> </a> 
are accepted at ICLR 2025
</li>

<li>Oct 2024. I will give a talk, <a><b>Algebraic Perspective of Neural Architecture</b> </a>, 
at the Department of ECE, Rice University
</li>

<li>Sep 2024. 1 paper <a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/577cd5863ec73be4e6871340be0936ae-Abstract-Conference.html"> <b> Monomial Matrix Group Equivariant Neural Functional Networks </b> </a> is accepted at NeurIPS 2024 </li>
</ul>
</details>



</p>
<h2><b>Selected Publications</b></h2>
<p>
</p>
<ul>
<li><p><a href="https://arxiv.org/abs/2509.11348" target=&ldquo;blank&rdquo;><b>On Linear Mode Connectivity of Mixture-of-Experts Architectures</b></a> <br />
<b>Viet-Hoang Tran</b>¹, Van Hoan Trinh¹, Khanh Vinh Bui¹, Tan M. Nguyen² <br />
NeurIPS 2025 <b>(oral)</b>

</p>
</li>
<li><p><a href="https://arxiv.org/abs/2410.04209" target=&ldquo;blank&rdquo;><b>Equivariant Neural Functional Networks for Transformers</b></a> <br />
<b>Viet-Hoang Tran</b>¹, Thieu N. Vo¹, An Nguyen The¹, Tho Tran Huu, Minh-Khoi Nguyen-Nhat, Thanh Tran, Duy-Tung Pham, Tan Minh Nguyen² <br />
ICLR 2025 <br />

</p>
</li>
<li><p><a href="https://arxiv.org/abs/2503.11050" target=&ldquo;blank&rdquo;><b>Distance-Based Tree-Sliced Wasserstein Distance</b></a> <br />
<b>Viet-Hoang Tran</b>¹, Khoi N.M. Nguyen¹, Trang Pham, Thanh T. Chu, Tam Le², Tan M. Nguyen² <br />
ICLR 2025 <br />

</p>
</li>
</ul>
<p>

</p>
<p>
<a href="publications.html"> 
<font color="black"> <span style="font-size: 0.7em; font-weight: bold;text-shadow: 0.7px 0 0 currentColor,
0.7px 0 0 currentColor,
0 0.7px 0 currentColor,
0 0.7px 0 currentColor;"> > <span style="margin-left:7px;"></span> </span> 
</font>
<font color="black"> <span style="font-size: 80%;"> all publications <span style="margin-left:7px;"></span> </span> 
</font>
</a>



</p>
<div id="footer">
<div id="footer-text">
Page generated 2026-02-11 09:08:52 PST, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</div>
</body>
</html>
